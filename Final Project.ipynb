{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are finding the points of topic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"[SOUND] This lecture is a overview of text retrieval methods. In the previous lecture, we introduced the problem of text retrieval. We explained that the main problem is the design of ranking function to rank documents for a query. In this lecture, we will give an overview of different ways of designing this ranking function. So the problem is the following. We have a query that has a sequence of words and the document that's also a sequence of words. And we hope to define a function f that can compute a score based on the query and document. So the main challenge you hear is with design a good ranking function that can rank all the relevant documents on top of all the non-relevant ones. Clearly, this means our function must be able to measure the likelihood that a document d is relevant to a query q. That also means we have to have some way to define relevance. In particular, in order to implement the program to do that, we have to have a computational definition of relevance. And we achieve this goal by designing a retrieval model, which gives us a formalization of relevance. Now, over many decades, researchers have designed many different kinds of retrieval models. And they fall into different categories. First, one family of the models are based on the similarity idea. Basically, we assume that if a document is more similar to the query than another document is, then we will say the first document is more relevant than the second one. So in this case, the ranking function is defined as the similarity between the query and the document. One well known example in this case is vector space model, which we will cover more in detail later in the lecture. A second kind of models are called probabilistic models. In this family of models, we follow a very different strategy, where we assume that queries and documents are all observations from random variables. And we assume there is a binary random variable called R here to indicate whether a document is relevant to a query. We then define the score of document with respect to a query as a probability that this random variable R is equal to 1, given a particular document query. There are different cases of such a general idea. One is classic probabilistic model, another is language model, yet another is divergence from randomness model. In a later lecture, we will talk more about one case, which is language model. A third kind of model are based on probabilistic inference. So here the idea is to associate uncertainty to inference rules, and we can then quantify the probability that we can show that the query follows from the document. Finally, there is also a family of models that are using axiomatic thinking. Here, an idea is to define a set of constraints that we hope a good retrieval function to satisfy. So in this case, the problem is to seek a good ranking function that can satisfy all the desired constraints. Interestingly, although these different models are based on different thinking, in the end, the retrieval function tends to be very similar. And these functions tend to also involve similar variables. So now let's take a look at the common form of a state of the art retrieval model and to examine some of the common ideas used in all these models. First, these models are all based on the assumption of using bag of words to represent text, and we explained this in the natural language processing lecture. Bag of words representation remains the main representation used in all the search engines. So with this assumption, the score of a query, like a presidential campaign news with respect to a document of d here, would be based on scores computed based on each individual word. And that means the score would depend on the score of each word, such as presidential, campaign, and news. Here, we can see there are three different components, each corresponding to how well the document matches each of the query words. Inside of these functions, we see a number of heuristics used. So for example, one factor that affects the function d here is how many times does the word presidential occur in the document? This is called a term frequency, or TF. We might also denote as c of presidential and d. In general, if the word occurs more frequently in the document, then the value of this function would be larger. Another factor is, how long is the document? And this is to use the document length for scoring. In general, if a term occurs in a long document many times, it's not as significant as if it occurred the same number of times in a short document. Because in a long document, any term is expected to occur more frequently. Finally, there is this factor called document frequency. That is, we also want to look at how often presidential occurs in the entire collection, and we call this document frequency, or df of presidential. And in some other models, we might also use a probability to characterize this information. So here, I show the probability of presidential in the collection. So all these are trying to characterize the popularity of the term in the collection. In general, matching a rare term in the collection is contributing more to the overall score than matching up common term. So this captures some of the main ideas used in pretty much older state of the art original models. So now, a natural question is, which model works the best? Now it turns out that many models work equally well. So here are a list of the four major models that are generally regarded as a state of the art original models, pivoted length normalization, BM25, query likelihood, PL2. When optimized, these models tend to perform similarly. And this was discussed in detail in this reference at the end of this lecture. Among all these, BM25 is probably the most popular. It's most likely that this has been used in virtually all the search engines, and you will also often see this method discussed in research papers. And we'll talk more about this method later in some other lectures. So, to summarize, the main points made in this lecture are first the design of a good ranking function pre-requires a computational definition of relevance, and we achieve this goal by designing appropriate retrieval model. Second, many models are equally effective, but we don't have a single winner yet. Researchers are still active and working on this problem, trying to find a truly optimal retrieval model. Finally, the state of the art ranking functions tend to rely on the following ideas. First, bag of words representation. Second, TF and document frequency of words. Such information is used in the weighting function to determine the overall contribution of matching a word and document length. These are often combined in interesting ways, and we'll discuss how exactly they are combined to rank documents in the lectures later. There are two suggested additional readings if you have time. The first is a paper where you can find the detailed discussion and comparison of multiple state of the art models. The second is a book with a chapter that gives a broad review of different retrieval models. [MUSIC]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SOUND] This lecture is a overview of text retrieval methods. In the previous lecture, we introduced the problem of text retrieval. We explained that the main problem is the design of ranking function to rank documents for a query. In this lecture, we will give an overview of different ways of designing this ranking function. So the problem is the following. We have a query that has a sequence of words and the document that's also a sequence of words. And we hope to define a function f that can compute a score based on the query and document. So the main challenge you hear is with design a good ranking function that can rank all the relevant documents on top of all the non-relevant ones. Clearly, this means our function must be able to measure the likelihood that a document d is relevant to a query q. That also means we have to have some way to define relevance. In particular, in order to implement the program to do that, we have to have a computational definition of relevance. And we achieve this goal by designing a retrieval model, which gives us a formalization of relevance. Now, over many decades, researchers have designed many different kinds of retrieval models. And they fall into different categories. First, one family of the models are based on the similarity idea. Basically, we assume that if a document is more similar to the query than another document is, then we will say the first document is more relevant than the second one. So in this case, the ranking function is defined as the similarity between the query and the document. One well known example in this case is vector space model, which we will cover more in detail later in the lecture. A second kind of models are called probabilistic models. In this family of models, we follow a very different strategy, where we assume that queries and documents are all observations from random variables. And we assume there is a binary random variable called R here to indicate whether a document is relevant to a query. We then define the score of document with respect to a query as a probability that this random variable R is equal to 1, given a particular document query. There are different cases of such a general idea. One is classic probabilistic model, another is language model, yet another is divergence from randomness model. In a later lecture, we will talk more about one case, which is language model. A third kind of model are based on probabilistic inference. So here the idea is to associate uncertainty to inference rules, and we can then quantify the probability that we can show that the query follows from the document. Finally, there is also a family of models that are using axiomatic thinking. Here, an idea is to define a set of constraints that we hope a good retrieval function to satisfy. So in this case, the problem is to seek a good ranking function that can satisfy all the desired constraints. Interestingly, although these different models are based on different thinking, in the end, the retrieval function tends to be very similar. And these functions tend to also involve similar variables. So now let's take a look at the common form of a state of the art retrieval model and to examine some of the common ideas used in all these models. First, these models are all based on the assumption of using bag of words to represent text, and we explained this in the natural language processing lecture. Bag of words representation remains the main representation used in all the search engines. So with this assumption, the score of a query, like a presidential campaign news with respect to a document of d here, would be based on scores computed based on each individual word. And that means the score would depend on the score of each word, such as presidential, campaign, and news. Here, we can see there are three different components, each corresponding to how well the document matches each of the query words. Inside of these functions, we see a number of heuristics used. So for example, one factor that affects the function d here is how many times does the word presidential occur in the document? This is called a term frequency, or TF. We might also denote as c of presidential and d. In general, if the word occurs more frequently in the document, then the value of this function would be larger. Another factor is, how long is the document? And this is to use the document length for scoring. In general, if a term occurs in a long document many times, it's not as significant as if it occurred the same number of times in a short document. Because in a long document, any term is expected to occur more frequently. Finally, there is this factor called document frequency. That is, we also want to look at how often presidential occurs in the entire collection, and we call this document frequency, or df of presidential. And in some other models, we might also use a probability to characterize this information. So here, I show the probability of presidential in the collection. So all these are trying to characterize the popularity of the term in the collection. In general, matching a rare term in the collection is contributing more to the overall score than matching up common term. So this captures some of the main ideas used in pretty much older state of the art original models. So now, a natural question is, which model works the best? Now it turns out that many models work equally well. So here are a list of the four major models that are generally regarded as a state of the art original models, pivoted length normalization, BM25, query likelihood, PL2. When optimized, these models tend to perform similarly. And this was discussed in detail in this reference at the end of this lecture. Among all these, BM25 is probably the most popular. It's most likely that this has been used in virtually all the search engines, and you will also often see this method discussed in research papers. And we'll talk more about this method later in some other lectures. So, to summarize, the main points made in this lecture are first the design of a good ranking function pre-requires a computational definition of relevance, and we achieve this goal by designing appropriate retrieval model. Second, many models are equally effective, but we don't have a single winner yet. Researchers are still active and working on this problem, trying to find a truly optimal retrieval model. Finally, the state of the art ranking functions tend to rely on the following ideas. First, bag of words representation. Second, TF and document frequency of words. Such information is used in the weighting function to determine the overall contribution of matching a word and document length. These are often combined in interesting ways, and we'll discuss how exactly they are combined to rank documents in the lectures later. There are two suggested additional readings if you have time. The first is a paper where you can find the detailed discussion and comparison of multiple state of the art models. The second is a book with a chapter that gives a broad review of different retrieval models. [MUSIC]\n"
     ]
    }
   ],
   "source": [
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Code acquired from https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb\n",
    "\n",
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "processed_docs.append(preprocess(transcript))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 abl\n",
      "1 achiev\n",
      "2 activ\n",
      "3 addit\n",
      "4 affect\n",
      "5 appropri\n",
      "6 associ\n",
      "7 assum\n",
      "8 assumpt\n",
      "9 axiomat\n",
      "10 base\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 3), (8, 2), (9, 1), (10, 7), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 4), (17, 2), (18, 1), (19, 5), (20, 1), (21, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 4), (27, 2), (28, 3), (29, 1), (30, 1), (31, 4), (32, 2), (33, 2), (34, 1), (35, 1), (36, 1), (37, 5), (38, 2), (39, 1), (40, 1), (41, 7), (42, 1), (43, 1), (44, 1), (45, 9), (46, 4), (47, 1), (48, 28), (49, 1), (50, 1), (51, 2), (52, 1), (53, 3), (54, 1), (55, 1), (56, 2), (57, 1), (58, 2), (59, 3), (60, 1), (61, 3), (62, 3), (63, 4), (64, 1), (65, 1), (66, 4), (67, 2), (68, 16), (69, 5), (70, 3), (71, 2), (72, 4), (73, 1), (74, 1), (75, 2), (76, 7), (77, 1), (78, 1), (79, 1), (80, 2), (81, 2), (82, 1), (83, 2), (84, 1), (85, 1), (86, 3), (87, 1), (88, 3), (89, 1), (90, 4), (91, 10), (92, 3), (93, 1), (94, 2), (95, 2), (96, 1), (97, 3), (98, 2), (99, 5), (100, 1), (101, 4), (102, 3), (103, 1), (104, 3), (105, 29), (106, 1), (107, 1), (108, 2), (109, 2), (110, 1), (111, 1), (112, 2), (113, 1), (114, 6), (115, 1), (116, 1), (117, 2), (118, 1), (119, 2), (120, 2), (121, 2), (122, 2), (123, 2), (124, 1), (125, 1), (126, 1), (127, 2), (128, 1), (129, 7), (130, 1), (131, 1), (132, 3), (133, 5), (134, 5), (135, 1), (136, 1), (137, 1), (138, 14), (139, 1), (140, 4), (141, 10), (142, 1), (143, 1), (144, 1), (145, 1), (146, 8), (147, 1), (148, 1), (149, 1), (150, 3), (151, 3), (152, 2), (153, 10), (154, 1), (155, 1), (156, 2), (157, 8), (158, 2), (159, 5), (160, 1), (161, 2), (162, 1), (163, 1), (164, 6), (165, 1), (166, 1), (167, 1), (168, 5), (169, 1), (170, 1), (171, 1), (172, 2), (173, 4), (174, 6), (175, 3), (176, 1), (177, 2), (178, 4), (179, 2), (180, 1), (181, 1), (182, 1), (183, 1), (184, 4), (185, 1), (186, 1), (187, 1), (188, 2), (189, 1), (190, 1), (191, 12), (192, 3)]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "print(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"abl\") appears 1 time.\n",
      "Word 1 (\"achiev\") appears 2 time.\n",
      "Word 2 (\"activ\") appears 1 time.\n",
      "Word 3 (\"addit\") appears 1 time.\n",
      "Word 4 (\"affect\") appears 1 time.\n",
      "Word 5 (\"appropri\") appears 1 time.\n",
      "Word 6 (\"associ\") appears 1 time.\n",
      "Word 7 (\"assum\") appears 3 time.\n",
      "Word 8 (\"assumpt\") appears 2 time.\n",
      "Word 9 (\"axiomat\") appears 1 time.\n",
      "Word 10 (\"base\") appears 7 time.\n",
      "Word 11 (\"basic\") appears 1 time.\n",
      "Word 12 (\"best\") appears 1 time.\n",
      "Word 13 (\"binari\") appears 1 time.\n",
      "Word 14 (\"book\") appears 1 time.\n",
      "Word 15 (\"broad\") appears 1 time.\n",
      "Word 16 (\"call\") appears 4 time.\n",
      "Word 17 (\"campaign\") appears 2 time.\n",
      "Word 18 (\"captur\") appears 1 time.\n",
      "Word 19 (\"case\") appears 5 time.\n",
      "Word 20 (\"categori\") appears 1 time.\n",
      "Word 21 (\"challeng\") appears 1 time.\n",
      "Word 22 (\"chapter\") appears 1 time.\n",
      "Word 23 (\"character\") appears 2 time.\n",
      "Word 24 (\"classic\") appears 1 time.\n",
      "Word 25 (\"clear\") appears 1 time.\n",
      "Word 26 (\"collect\") appears 4 time.\n",
      "Word 27 (\"combin\") appears 2 time.\n",
      "Word 28 (\"common\") appears 3 time.\n",
      "Word 29 (\"comparison\") appears 1 time.\n",
      "Word 30 (\"compon\") appears 1 time.\n",
      "Word 31 (\"comput\") appears 4 time.\n",
      "Word 32 (\"constraint\") appears 2 time.\n",
      "Word 33 (\"contribut\") appears 2 time.\n",
      "Word 34 (\"correspond\") appears 1 time.\n",
      "Word 35 (\"cover\") appears 1 time.\n",
      "Word 36 (\"decad\") appears 1 time.\n",
      "Word 37 (\"defin\") appears 5 time.\n",
      "Word 38 (\"definit\") appears 2 time.\n",
      "Word 39 (\"denot\") appears 1 time.\n",
      "Word 40 (\"depend\") appears 1 time.\n",
      "Word 41 (\"design\") appears 7 time.\n",
      "Word 42 (\"desir\") appears 1 time.\n",
      "Word 43 (\"detail\") appears 1 time.\n",
      "Word 44 (\"determin\") appears 1 time.\n",
      "Word 45 (\"differ\") appears 9 time.\n",
      "Word 46 (\"discuss\") appears 4 time.\n",
      "Word 47 (\"diverg\") appears 1 time.\n",
      "Word 48 (\"document\") appears 28 time.\n",
      "Word 49 (\"dont\") appears 1 time.\n",
      "Word 50 (\"effect\") appears 1 time.\n",
      "Word 51 (\"engin\") appears 2 time.\n",
      "Word 52 (\"entir\") appears 1 time.\n",
      "Word 53 (\"equal\") appears 3 time.\n",
      "Word 54 (\"exact\") appears 1 time.\n",
      "Word 55 (\"examin\") appears 1 time.\n",
      "Word 56 (\"exampl\") appears 2 time.\n",
      "Word 57 (\"expect\") appears 1 time.\n",
      "Word 58 (\"explain\") appears 2 time.\n",
      "Word 59 (\"factor\") appears 3 time.\n",
      "Word 60 (\"fall\") appears 1 time.\n",
      "Word 61 (\"famili\") appears 3 time.\n",
      "Word 62 (\"final\") appears 3 time.\n",
      "Word 63 (\"follow\") appears 4 time.\n",
      "Word 64 (\"form\") appears 1 time.\n",
      "Word 65 (\"formal\") appears 1 time.\n",
      "Word 66 (\"frequenc\") appears 4 time.\n",
      "Word 67 (\"frequent\") appears 2 time.\n",
      "Word 68 (\"function\") appears 16 time.\n",
      "Word 69 (\"general\") appears 5 time.\n",
      "Word 70 (\"give\") appears 3 time.\n",
      "Word 71 (\"goal\") appears 2 time.\n",
      "Word 72 (\"good\") appears 4 time.\n",
      "Word 73 (\"hear\") appears 1 time.\n",
      "Word 74 (\"heurist\") appears 1 time.\n",
      "Word 75 (\"hope\") appears 2 time.\n",
      "Word 76 (\"idea\") appears 7 time.\n",
      "Word 77 (\"implement\") appears 1 time.\n",
      "Word 78 (\"indic\") appears 1 time.\n",
      "Word 79 (\"individu\") appears 1 time.\n",
      "Word 80 (\"infer\") appears 2 time.\n",
      "Word 81 (\"inform\") appears 2 time.\n",
      "Word 82 (\"insid\") appears 1 time.\n",
      "Word 83 (\"interest\") appears 2 time.\n",
      "Word 84 (\"introduc\") appears 1 time.\n",
      "Word 85 (\"involv\") appears 1 time.\n",
      "Word 86 (\"kind\") appears 3 time.\n",
      "Word 87 (\"know\") appears 1 time.\n",
      "Word 88 (\"languag\") appears 3 time.\n",
      "Word 89 (\"larger\") appears 1 time.\n",
      "Word 90 (\"later\") appears 4 time.\n",
      "Word 91 (\"lectur\") appears 10 time.\n",
      "Word 92 (\"length\") appears 3 time.\n",
      "Word 93 (\"let\") appears 1 time.\n",
      "Word 94 (\"like\") appears 2 time.\n",
      "Word 95 (\"likelihood\") appears 2 time.\n",
      "Word 96 (\"list\") appears 1 time.\n",
      "Word 97 (\"long\") appears 3 time.\n",
      "Word 98 (\"look\") appears 2 time.\n",
      "Word 99 (\"main\") appears 5 time.\n",
      "Word 100 (\"major\") appears 1 time.\n",
      "Word 101 (\"match\") appears 4 time.\n",
      "Word 102 (\"mean\") appears 3 time.\n",
      "Word 103 (\"measur\") appears 1 time.\n",
      "Word 104 (\"method\") appears 3 time.\n",
      "Word 105 (\"model\") appears 29 time.\n",
      "Word 106 (\"multipl\") appears 1 time.\n",
      "Word 107 (\"music\") appears 1 time.\n",
      "Word 108 (\"natur\") appears 2 time.\n",
      "Word 109 (\"news\") appears 2 time.\n",
      "Word 110 (\"nonrelev\") appears 1 time.\n",
      "Word 111 (\"normal\") appears 1 time.\n",
      "Word 112 (\"number\") appears 2 time.\n",
      "Word 113 (\"observ\") appears 1 time.\n",
      "Word 114 (\"occur\") appears 6 time.\n",
      "Word 115 (\"older\") appears 1 time.\n",
      "Word 116 (\"one\") appears 1 time.\n",
      "Word 117 (\"optim\") appears 2 time.\n",
      "Word 118 (\"order\") appears 1 time.\n",
      "Word 119 (\"origin\") appears 2 time.\n",
      "Word 120 (\"overal\") appears 2 time.\n",
      "Word 121 (\"overview\") appears 2 time.\n",
      "Word 122 (\"paper\") appears 2 time.\n",
      "Word 123 (\"particular\") appears 2 time.\n",
      "Word 124 (\"perform\") appears 1 time.\n",
      "Word 125 (\"pivot\") appears 1 time.\n",
      "Word 126 (\"point\") appears 1 time.\n",
      "Word 127 (\"popular\") appears 2 time.\n",
      "Word 128 (\"prerequir\") appears 1 time.\n",
      "Word 129 (\"presidenti\") appears 7 time.\n",
      "Word 130 (\"pretti\") appears 1 time.\n",
      "Word 131 (\"previous\") appears 1 time.\n",
      "Word 132 (\"probabilist\") appears 3 time.\n",
      "Word 133 (\"probabl\") appears 5 time.\n",
      "Word 134 (\"problem\") appears 5 time.\n",
      "Word 135 (\"process\") appears 1 time.\n",
      "Word 136 (\"program\") appears 1 time.\n",
      "Word 137 (\"quantifi\") appears 1 time.\n",
      "Word 138 (\"queri\") appears 14 time.\n",
      "Word 139 (\"question\") appears 1 time.\n",
      "Word 140 (\"random\") appears 4 time.\n",
      "Word 141 (\"rank\") appears 10 time.\n",
      "Word 142 (\"rare\") appears 1 time.\n",
      "Word 143 (\"read\") appears 1 time.\n",
      "Word 144 (\"refer\") appears 1 time.\n",
      "Word 145 (\"regard\") appears 1 time.\n",
      "Word 146 (\"relev\") appears 8 time.\n",
      "Word 147 (\"reli\") appears 1 time.\n",
      "Word 148 (\"remain\") appears 1 time.\n",
      "Word 149 (\"repres\") appears 1 time.\n",
      "Word 150 (\"represent\") appears 3 time.\n",
      "Word 151 (\"research\") appears 3 time.\n",
      "Word 152 (\"respect\") appears 2 time.\n",
      "Word 153 (\"retriev\") appears 10 time.\n",
      "Word 154 (\"review\") appears 1 time.\n",
      "Word 155 (\"rule\") appears 1 time.\n",
      "Word 156 (\"satisfi\") appears 2 time.\n",
      "Word 157 (\"score\") appears 8 time.\n",
      "Word 158 (\"search\") appears 2 time.\n",
      "Word 159 (\"second\") appears 5 time.\n",
      "Word 160 (\"seek\") appears 1 time.\n",
      "Word 161 (\"sequenc\") appears 2 time.\n",
      "Word 162 (\"short\") appears 1 time.\n",
      "Word 163 (\"signific\") appears 1 time.\n",
      "Word 164 (\"similar\") appears 6 time.\n",
      "Word 165 (\"singl\") appears 1 time.\n",
      "Word 166 (\"sound\") appears 1 time.\n",
      "Word 167 (\"space\") appears 1 time.\n",
      "Word 168 (\"state\") appears 5 time.\n",
      "Word 169 (\"strategi\") appears 1 time.\n",
      "Word 170 (\"suggest\") appears 1 time.\n",
      "Word 171 (\"summar\") appears 1 time.\n",
      "Word 172 (\"talk\") appears 2 time.\n",
      "Word 173 (\"tend\") appears 4 time.\n",
      "Word 174 (\"term\") appears 6 time.\n",
      "Word 175 (\"text\") appears 3 time.\n",
      "Word 176 (\"that\") appears 1 time.\n",
      "Word 177 (\"think\") appears 2 time.\n",
      "Word 178 (\"time\") appears 4 time.\n",
      "Word 179 (\"tri\") appears 2 time.\n",
      "Word 180 (\"truli\") appears 1 time.\n",
      "Word 181 (\"turn\") appears 1 time.\n",
      "Word 182 (\"uncertainti\") appears 1 time.\n",
      "Word 183 (\"valu\") appears 1 time.\n",
      "Word 184 (\"variabl\") appears 4 time.\n",
      "Word 185 (\"vector\") appears 1 time.\n",
      "Word 186 (\"virtual\") appears 1 time.\n",
      "Word 187 (\"want\") appears 1 time.\n",
      "Word 188 (\"way\") appears 2 time.\n",
      "Word 189 (\"weight\") appears 1 time.\n",
      "Word 190 (\"winner\") appears 1 time.\n",
      "Word 191 (\"word\") appears 12 time.\n",
      "Word 192 (\"work\") appears 3 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "document_num = 0\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, \n",
    "#                                    num_topics = 10, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 50)\n",
    "\n",
    "# LDA multicore \n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "# TODO\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 1, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 3,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.042*\"model\" + 0.041*\"document\" + 0.024*\"function\" + 0.021*\"queri\" + 0.018*\"word\" + 0.015*\"retriev\" + 0.015*\"rank\" + 0.015*\"lectur\" + 0.014*\"differ\" + 0.013*\"score\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textsplit\n",
      "  Downloading textsplit-0.5-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: numpy>=1.13.1 in c:\\users\\urvi\\anaconda3\\lib\\site-packages (from textsplit) (1.19.2)\n",
      "Requirement already satisfied: nose>=1.3.7 in c:\\users\\urvi\\anaconda3\\lib\\site-packages (from textsplit) (1.3.7)\n",
      "Installing collected packages: textsplit\n",
      "Successfully installed textsplit-0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textsplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I'd like an apple\", \n",
    "\"An apple a day keeps the doctor away\", \n",
    "\"Never compare an apple to an orange\", \n",
    "\"I prefer scikit-learn to Orange\", \n",
    " \"The scikit-learn docs are Orange and Blue\"]   \n",
    "\n",
    "vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
    "tfidf = vect.fit_transform(corpus)                                                                                                                                                                                                                       \n",
    "pairwise_similarity = tfidf * tfidf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.17668795 0.27056873 0.         0.        ]\n",
      " [0.17668795 1.         0.15439436 0.         0.        ]\n",
      " [0.27056873 0.15439436 1.         0.19635649 0.16815247]\n",
      " [0.         0.         0.19635649 1.         0.54499756]\n",
      " [0.         0.         0.16815247 0.54499756 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(pairwise_similarity.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
